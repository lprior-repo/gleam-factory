#!/bin/bash
#
# TDD Loop: Adversarial AI-driven Test-Driven Development
#
# Two AI sessions with filesystem-enforced separation:
# - Auditor: writes tests (can't touch src/)
# - Implementer: writes code (can't touch test/)
#
# Usage: ./tdd-loop "Feature description or requirements"
#

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configuration
SRC_DIR="src"
TEST_DIR="test"
MAX_IMPL_ATTEMPTS=3
MAX_ITERATIONS=10  # Auto-stop after this many iterations
AUTO_MODE=true     # No prompts, fully autonomous

# Handle requirements from file or argument
if [ -f "$1" ]; then
    REQUIREMENTS=$(cat "$1")
    REQUIREMENTS_SOURCE="file: $1"
else
    REQUIREMENTS="$1"
    REQUIREMENTS_SOURCE="argument"
fi

# Detect language from project files
detect_language() {
    if [ -f "gleam.toml" ]; then
        echo "gleam"
    elif [ -f "go.mod" ]; then
        echo "go"
    elif [ -f "Cargo.toml" ]; then
        echo "rust"
    elif [ -f "pyproject.toml" ] || [ -f "setup.py" ]; then
        echo "python"
    else
        echo "unknown"
    fi
}

LANG=$(detect_language)
TEST_CMD="gleam test"
case "$LANG" in
    gleam) TEST_CMD="gleam test" ;;
    go) TEST_CMD="go test ./..." ;;
    rust) TEST_CMD="cargo test" ;;
    python) TEST_CMD="pytest" ;;
esac

# Session tracking (for resume capability)
SESSION_FILE=".tdd-session"
AUDITOR_SESSION=""
IMPLEMENTER_SESSION=""

#---------------------------------------------------------------------------
# Utility functions
#---------------------------------------------------------------------------

log_phase() {
    echo ""
    echo -e "${BLUE}════════════════════════════════════════════════════════════${NC}"
    echo -e "${BLUE}  $1${NC}"
    echo -e "${BLUE}════════════════════════════════════════════════════════════${NC}"
    echo ""
}

log_info() {
    echo -e "${CYAN}→ $1${NC}"
}

log_success() {
    echo -e "${GREEN}✓ $1${NC}"
}

log_warning() {
    echo -e "${YELLOW}⚠ $1${NC}"
}

log_error() {
    echo -e "${RED}✗ $1${NC}"
}

#---------------------------------------------------------------------------
# Lock management
#---------------------------------------------------------------------------

lock_all() {
    log_info "Locking all source and test files..."
    chmod -R a-w "$SRC_DIR/" 2>/dev/null || true
    chmod -R a-w "$TEST_DIR/" 2>/dev/null || true
}

unlock_src() {
    log_info "Unlocking src/ for implementer..."
    chmod -R u+w "$SRC_DIR/"
}

lock_src() {
    log_info "Locking src/..."
    chmod -R a-w "$SRC_DIR/"
}

unlock_tests() {
    log_info "Unlocking test/ for auditor..."
    chmod -R u+w "$TEST_DIR/"
}

lock_tests() {
    log_info "Locking test/..."
    chmod -R a-w "$TEST_DIR/"
}

#---------------------------------------------------------------------------
# Test execution
#---------------------------------------------------------------------------

run_tests() {
    log_info "Running tests ($TEST_CMD)..."
    if $TEST_CMD 2>&1; then
        return 0
    else
        return 1
    fi
}

# Capture test output for passing to implementer
capture_test_output() {
    $TEST_CMD 2>&1 || true
}

#---------------------------------------------------------------------------
# AI Session management
#---------------------------------------------------------------------------

# Auditor prompt: writes tests, can see src (read-only) and requirements
auditor_prompt() {
    local iteration="$1"
    local last_impl_result="$2"

    cat <<EOF
You are the AUDITOR in a TDD loop. Your job is to write tests.

PROJECT: $LANG project
TEST COMMAND: $TEST_CMD

REQUIREMENTS:
$REQUIREMENTS

ITERATION: $iteration

YOUR CONSTRAINTS:
- You can ONLY modify files in test/
- You CANNOT modify files in src/ (read-only, filesystem enforced)
- Write ONE focused test at a time
- The test should FAIL initially (red phase of TDD)

YOUR TASK:
1. Read the current src/ code to understand what exists
2. Read the current tests to understand what's covered
3. Write EXACTLY ONE test that:
   - Tests ONE specific behavior or edge case
   - Drives better code design (not just coverage)
   - Forces the implementer to write clean, focused code
   - Follows testing best practices (arrange-act-assert, descriptive names)

TEST DESIGN PRINCIPLES:
- Each test should validate ONE thing
- Test names should describe the behavior being tested
- Tests should be independent and not rely on each other
- Prefer testing behavior over implementation details
- Edge cases matter: empty inputs, boundaries, error conditions

IMPORTANT: Write ONLY ONE test per iteration. If ALL requirements are fully tested and implemented, output EXACTLY:
REQUIREMENTS_COMPLETE
and do not write any new tests.

$(if [ -n "$last_impl_result" ]; then
    echo "LAST IMPLEMENTATION RESULT:"
    echo "$last_impl_result"
    echo ""
    echo "If tests passed, write the next test for uncovered requirements."
    echo "If tests failed due to test bugs, fix your tests."
fi)

OUTPUT: Only modify test files. Explain what you're testing and why.
EOF
}

# Implementer prompt: makes tests pass, can only see tests
implementer_prompt() {
    local test_output="$1"
    local attempt="$2"

    cat <<EOF
You are the IMPLEMENTER in a TDD loop. Your job is to make tests pass.

PROJECT: $LANG project
TEST COMMAND: $TEST_CMD

YOUR CONSTRAINTS:
- You can ONLY modify files in src/
- You CANNOT modify files in test/ (read-only, filesystem enforced)
- Write the MINIMUM code to make tests pass
- Follow existing patterns in the codebase

CURRENT TEST OUTPUT:
$test_output

ATTEMPT: $attempt of $MAX_IMPL_ATTEMPTS

YOUR TASK:
1. Read the failing test to understand what's expected
2. Read the current src/ code
3. Write the MINIMUM code to make ONLY this test pass
4. Do NOT over-engineer or add untested features
5. Each function should do ONE thing well

IMPLEMENTATION PRINCIPLES:
- Write only what the test demands - nothing more
- Keep functions small and focused
- Use descriptive names that explain intent
- Handle the specific case the test covers
- Let the next test drive the next piece of functionality

OUTPUT: Only modify src files. Explain what you implemented and why.
EOF
}

#---------------------------------------------------------------------------
# Main TDD Loop
#---------------------------------------------------------------------------

run_auditor() {
    local iteration="$1"
    local last_result="$2"
    local prompt

    log_phase "AUDITOR PHASE (Iteration $iteration)"

    # Unlock tests, lock src
    lock_src
    unlock_tests

    prompt=$(auditor_prompt "$iteration" "$last_result")

    log_info "Invoking auditor AI (new session)..."
    # Auditor can only use tools on test/, explicitly block src/
    # --session-id ensures fresh session each time
    AUDITOR_SESSION_ID=$(uuidgen)
    auditor_output=$(claude -p \
        --session-id "$AUDITOR_SESSION_ID" \
        --allowedTools "Read Write Edit Glob Grep Bash(gleam:*)" \
        --append-system-prompt "You are the AUDITOR. You may ONLY modify files in test/. Any attempt to modify src/ will fail. Focus on writing ONE failing test. If all requirements are met, output REQUIREMENTS_COMPLETE." \
        "$prompt" 2>&1)

    echo "$auditor_output"

    # Lock tests when done
    lock_tests

    # Check if auditor says we're done
    if echo "$auditor_output" | grep -q "REQUIREMENTS_COMPLETE"; then
        log_success "Auditor reports all requirements are complete!"
        return 1  # Signal completion
    fi

    log_success "Auditor phase complete"
    return 0
}

# Reviewer prompt: cleans up code after TDD is complete
reviewer_prompt() {
    cat <<EOF
You are the REVIEWER. The TDD loop has completed and all tests pass.

PROJECT: $LANG project
TEST COMMAND: $TEST_CMD

ORIGINAL REQUIREMENTS:
$REQUIREMENTS

YOUR TASK:
1. Review all code in src/ and test/ that was created/modified
2. Look for:
   - Code duplication that can be refactored
   - Poor naming that should be improved
   - Missing or incorrect documentation
   - Type safety improvements
   - Idiomatic $LANG patterns that should be applied
   - Any obvious bugs or edge cases the tests missed
3. Make improvements while ensuring ALL TESTS STILL PASS
4. Run '$TEST_CMD' after your changes to verify

CONSTRAINTS:
- You may modify both src/ and test/
- All tests MUST still pass after your changes
- Focus on code quality, not adding new features
- Be conservative - only change what clearly needs improvement

OUTPUT: Describe what you reviewed and what changes you made (if any).
EOF
}

run_reviewer() {
    log_phase "REVIEWER PHASE"

    # Unlock both for reviewer
    unlock_src
    unlock_tests

    local prompt
    prompt=$(reviewer_prompt)

    log_info "Invoking reviewer AI (new session)..."
    REVIEWER_SESSION_ID=$(uuidgen)
    claude -p \
        --session-id "$REVIEWER_SESSION_ID" \
        --allowedTools "Read Write Edit Glob Grep Bash(gleam:*)" \
        --append-system-prompt "You are the REVIEWER. You may modify both src/ and test/. All tests MUST pass after your changes. Focus on code quality improvements only." \
        "$prompt" 2>&1

    # Verify tests still pass after review
    log_info "Verifying tests still pass after review..."
    if run_tests; then
        log_success "Reviewer phase complete - all tests still pass"
    else
        log_error "Reviewer broke tests! Reverting..."
        # Could add git checkout here to revert, for now just warn
        log_warning "Tests are failing after review - manual intervention needed"
    fi

    lock_src
    lock_tests
}

run_implementer() {
    local test_output="$1"
    local attempt=1
    local result

    log_phase "IMPLEMENTER PHASE"

    while [ $attempt -le $MAX_IMPL_ATTEMPTS ]; do
        log_info "Implementation attempt $attempt of $MAX_IMPL_ATTEMPTS"

        # Unlock src, lock tests
        lock_tests
        unlock_src

        prompt=$(implementer_prompt "$test_output" "$attempt")

        log_info "Invoking implementer AI (new session)..."
        # Implementer can only use tools on src/, explicitly block test/
        # --session-id ensures fresh session each time
        IMPLEMENTER_SESSION_ID=$(uuidgen)
        claude -p \
            --session-id "$IMPLEMENTER_SESSION_ID" \
            --allowedTools "Read Write Edit Glob Grep Bash(gleam:*)" \
            --append-system-prompt "You are the IMPLEMENTER. You may ONLY modify files in src/. Any attempt to modify test/ will fail. Write MINIMUM code to pass the test." \
            "$prompt" 2>&1

        # Lock src when done
        lock_src

        # Run tests
        log_info "Checking if tests pass..."
        if run_tests; then
            log_success "Tests pass!"
            return 0
        else
            log_warning "Tests still failing"
            test_output=$(capture_test_output)
            attempt=$((attempt + 1))
        fi
    done

    log_error "Implementer failed after $MAX_IMPL_ATTEMPTS attempts"
    return 1
}

main() {
    if [ -z "$REQUIREMENTS" ]; then
        echo "Usage: ./tdd-loop \"Feature requirements\""
        echo ""
        echo "Example:"
        echo "  ./tdd-loop \"Add a function to validate email addresses\""
        exit 1
    fi

    log_phase "TDD LOOP STARTING"
    echo "Language: $LANG"
    echo "Test command: $TEST_CMD"
    echo "Requirements source: $REQUIREMENTS_SOURCE"
    echo ""
    echo "Requirements:"
    echo "$REQUIREMENTS"
    echo ""

    # Initial lock
    lock_all

    local iteration=1
    local last_impl_result=""

    while true; do
        log_phase "TDD ITERATION $iteration"

        # Phase 1: Auditor writes/updates test
        if ! run_auditor "$iteration" "$last_impl_result"; then
            # Auditor signaled completion
            log_phase "TDD LOOP COMPLETE - ALL REQUIREMENTS MET"
            log_success "Completed after $iteration iterations"

            # Now run the reviewer
            run_reviewer

            # Unlock everything at end
            chmod -R u+w "$SRC_DIR/" 2>/dev/null || true
            chmod -R u+w "$TEST_DIR/" 2>/dev/null || true

            exit 0
        fi

        # Check if new test fails (as expected for red phase)
        log_info "Verifying test is RED (failing)..."
        test_output=$(capture_test_output)

        if run_tests; then
            log_warning "Tests already pass - auditor should write a NEW failing test"
            log_info "Continuing to next iteration..."
            last_impl_result="All tests passed. Write a new failing test for uncovered requirements."
            iteration=$((iteration + 1))
            continue
        fi

        log_success "Test is RED - proceeding to implementation"

        # Phase 2: Implementer makes it pass
        if run_implementer "$test_output"; then
            last_impl_result="Tests passed. Ready for next test."
        else
            last_impl_result="Implementation failed. Test output: $test_output"
            log_error "Implementation phase failed"
        fi

        # Auto mode: continue to next iteration
        iteration=$((iteration + 1))

        # Check if we've hit max iterations
        if [ $iteration -gt $MAX_ITERATIONS ]; then
            log_phase "MAX ITERATIONS REACHED ($MAX_ITERATIONS)"
            log_success "Completed $((iteration - 1)) iterations"

            # Unlock everything at end
            chmod -R u+w "$SRC_DIR/" 2>/dev/null || true
            chmod -R u+w "$TEST_DIR/" 2>/dev/null || true

            exit 0
        fi

        log_info "Automatically continuing to iteration $iteration..."
    done
}

# Cleanup on exit
cleanup() {
    log_info "Cleaning up locks..."
    chmod -R u+w "$SRC_DIR/" 2>/dev/null || true
    chmod -R u+w "$TEST_DIR/" 2>/dev/null || true
}

trap cleanup EXIT

# Run
main
