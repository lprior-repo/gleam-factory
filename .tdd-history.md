# TDD-TCR-REFACTOR Session History

This file tracks the reasoning from each agent to provide context to subsequent agents.

---


## Iteration 0 - SESSION START

Requirements: Enhance ALL modules in the factory-gleam codebase with targeted improvements:

AUDIT MODULE (src/audit.gleam):
1. Add audit_trail(bead: Bead, action: String) -> String
   - Format: '[timestamp] action: bead_slug'
2. Add validate_audit_entry(entry: String) -> Result(Nil, String)

CLI MODULE (src/cli.gleam):
3. Add parse_args(args: List(String)) -> Result(Command, String)
   - Pure function (testable without argv)
4. Add short flags: -s=--slug, -p=--priority

CONFIG MODULE (src/config.gleam):
5. Add with_data_dir(config: Config, dir: String) -> Config
6. Add merge_configs(base: Config, override: Config) -> Config

DOMAIN MODULE (src/domain.gleam):
7. Add bead_age_days(bead: Bead) -> Int
8. Add is_stale(bead: Bead, max_days: Int) -> Bool

ERRORS MODULE (src/errors.gleam):
9. Add format_error_context(err: String, file: String, line: Int) -> String
10. Add chain_errors(errors: List(String)) -> String

INTEGRATION MODULE (src/integration.gleam):
11. Add retry_with_backoff(f: fn() -> Result(a, e), retries: Int) -> Result(a, e)

PERSISTENCE MODULE (src/persistence.gleam):
12. Add backup_bead(bead: Bead, backup_dir: String) -> Result(Nil, String)

PROCESS MODULE (src/process.gleam):
13. Add run_with_timeout(cmd: String, timeout_ms: Int) -> Result(String, String)
14. Add capture_stderr(cmd: String) -> Result(#(String, String), String)

REPO MODULE (src/repo.gleam):
15. Add get_remote_url() -> Result(String, String)
16. Add is_clean() -> Result(Bool, String)

STAGES MODULE (src/stages.gleam):
17. Add validate_stage_transition(from: Stage, to: Stage) -> Result(Nil, String)

TCR MODULE (src/tcr.gleam):
18. Add tcr_stats() -> #(Int, Int) - returns (commits, reverts)

UTILS MODULE (src/utils.gleam):
19. Add truncate_string(s: String, max_len: Int) -> String
20. Add pad_left(s: String, width: Int, char: String) -> String

VALIDATION MODULE (src/validation.gleam):
21. Add validate_slug_format(slug: String) -> Result(String, String)
   - Must start with letter, only alphanumeric/underscores, max 50 chars
22. Add validate_priority(p: String) -> Result(Priority, String)
   - Accept P1/P2/P3, p1/p2/p3, 1/2/3

WORKTREE MODULE (src/worktree.gleam):
23. Add list_worktrees() -> Result(List(String), String)
24. Add cleanup_stale_worktrees(max_age_days: Int) -> Result(Int, String)

These span ALL 15 source modules with 24 functions total.

---

## Iteration 1 - AUDITOR

Excellent! Let me provide a summary of what I've done:

## TEST 1 COMPLETE: validate_slug_format validation

I've written a comprehensive test suite for the `validate_slug_format` function that will be in the new `validation` module. This test covers:

### Test Cases Written:
1. **Valid cases:**
   - `feature_123` - starts with letter, alphanumeric + underscore
   - `task_1_a_b_c` - multiple underscores and numbers
   - `a` + 49 `b`s = 50 char slug - at max length boundary

2. **Invalid cases (error conditions):**
   - `123_feature` - starts with number (must start with letter)
   - `_feature` - starts with underscore (must start with letter)
   - `feature-name` - contains hyphen (only alphanumeric and underscore allowed)
   - `feature@name` - contains special character
   - `a` + 50 `b`s = 51 char slug - exceeds 50 char max
   - Empty string

### Why This Test Design:
- **Tests ONE behavior**: Pure validation of slug format constraints
- **Drives clean design**: Forces implementer to write a focused validation function with clear rules
- **Comprehensive edge cases**: Covers all constraint boundaries (start char, allowed chars, max length)
- **Independent tests**: Each test is self-contained with a single assertion
- **Descriptive names**: Each test name clearly describes what it validates
- **Currently FAILS** ✓ - The validation module doesn't exist yet, so tests fail (red phase of TDD)

### Next Step:
The implementer will create `src/validation.gleam` with a `pub fn validate_slug_format(slug: String) -> Result(String, String)` function that validates these exact rules. The function should return `Ok(slug)` if valid, or `Error(reason)` if invalid.

---

## Iteration 1 (attempt 1) - IMPLEMENTER

Perfect! The tests are passing. Let me provide the summary:

## Summary

**What the test is asking for:**
The test suite requires a new `validation` module with a `validate_slug_format` function that validates slug format according to specific rules: must start with a letter, can only contain letters/numbers/underscores, max 50 characters, and cannot be empty.

**What I implemented:**
Created `src/validation.gleam` with a `validate_slug_format` function that validates all the test requirements. The implementation includes:
- Length validation (non-empty, max 50 chars)
- First character validation (must be a letter)
- Character set validation (only alphanumeric and underscore allowed)
- Helper functions to check if characters are valid

**Why this approach:**
The implementation uses simple pattern matching and recursive character checking, which is idiomatic in Gleam. It returns `Ok(slug)` for valid input and `Error(String)` for invalid cases, matching the Result type expected by the tests. All 24 tests pass, including the 9 new validation tests.

---

## Iteration 2 - AUDITOR

Perfect! The tests fail because the function doesn't exist. This is exactly what we want - the tests are now driving the design. The implementer will need to:

1. Define a `Priority` type in the validation module
2. Implement the `validate_priority` function that accepts P1, P2, P3, p1, p2, p3, 1, 2, 3 and rejects invalid inputs

Let me document what the test is testing:

## ITERATION 2 - TEST: validate_priority

**What's being tested:** The `validate_priority` function in the validation module

**Why this test matters:**
- Priority is a core domain concept used throughout the system (CLI, persistence, filtering)
- The function must accept multiple formats: uppercase (P1/P2/P3), lowercase (p1/p2/p3), and numeric (1/2/3)
- Invalid priorities must be rejected to prevent bad data from entering the system
- This test drives the creation of a `Priority` type that makes illegal states unrepresentable

**Test Design:**
- `validate_priority_accepts_uppercase_p1_test`: Valid priority in uppercase format returns Ok
- `validate_priority_accepts_lowercase_p1_test`: Valid priority in lowercase format returns Ok
- `validate_priority_accepts_numeric_1_test`: Valid priority in numeric format returns Ok
- `validate_priority_rejects_invalid_p4_test`: Invalid priority (P4) returns Error
- `validate_priority_rejects_empty_string_test`: Empty string returns Error

**Expected Implementation:**
The implementer should:
1. Define a `Priority` type with three variants: P1, P2, P3
2. Implement `validate_priority(p: String) -> Result(Priority, String)` that:
   - Normalizes input (lowercase/uppercase/numeric formats)
   - Returns Ok with the Priority variant if valid (P1, P2, or P3)
   - Returns Error with descriptive message if invalid

**Current Status:** Tests are RED (failing) - ready for implementation phase

---

## Iteration 2 (attempt 1) - IMPLEMENTER

Perfect! Here's what was implemented:

## What the test is asking for
The test requires a `validate_priority` function that accepts priority strings in multiple formats (uppercase P1/P2/P3, lowercase p1/p2/p3, or numeric 1/2/3) and rejects invalid inputs like "P4" or empty strings.

## What I implemented
1. **Added `Priority` type** with three variants: `P1`, `P2`, `P3` (lines 3-7 in validation.gleam)
2. **Implemented `validate_priority` function** that:
   - Normalizes input using `string.lowercase()` to handle both uppercase and lowercase formats
   - Maps "p1"/"1" → P1, "p2"/"2" → P2, "p3"/"3" → P3
   - Returns `Error("Priority cannot be empty")` for empty strings
   - Returns `Error("Invalid priority")` for any other input like "p4"

## Why this approach
This minimal implementation directly satisfies all 5 test cases by normalizing input and pattern matching on the lowercase representation. The `Priority` type makes the domain concept explicit while the function handles the validation logic cleanly with descriptive error messages.

---

## Iteration 3 - AUDITOR

Perfect! The test fails because `parse_args` doesn't exist yet. This is exactly what we want - a failing test that will drive implementation. The error message is clear: the `cli` module doesn't have `parse_args`.

Let me verify the test compiles and fails properly by checking the exact error:

The test clearly shows:
1. The test imports `cli` module ✓
2. The test calls `cli.parse_args(["new", "-s", "my_task"])` ✓
3. The test expects an Ok result (pure parsing should succeed for valid input) ✓
4. The function doesn't exist yet, so the test fails with a clear message ✓

This test is well-designed because:

**What it tests:** The pure CLI argument parser's ability to handle the short flag `-s` as an alias for `--slug`.

**Why this matters:** 
- It forces the implementer to create a pure `parse_args` function (testable, no argv dependency)
- It tests short flag support which is requirement 4 ("-s=--slug")
- The test is simple and focused - just one case
- It's independent of any external state

**Expected behavior:**
The implementer will need to:
1. Add `pub fn parse_args(args: List(String)) -> Result(Command, String)` to the cli module
2. Implement flag normalization to convert `-s` to `--slug` 
3. Handle the `["new", "-s", "my_task"]` case properly

**Next steps for implementer:**
They'll implement this function, and once it passes, I can write additional tests for other short flags (`-p` for `--priority`), long flags, and edge cases.

REQUIREMENTS STATUS:
- Requirements 1-2 (audit_trail, validate_audit_entry): Not started
- Requirements 3-4 (parse_args, short flags): **ITERATION 3 - Testing now** ✓
- Requirements 5-6 (with_data_dir, merge_configs): Not started
- Requirements 7-8 (bead_age_days, is_stale): Not started
- Requirements 9-10 (format_error_context, chain_errors): Not started
- Requirements 11 (retry_with_backoff): Not started
- Requirements 12 (backup_bead): Not started
- Requirements 13-14 (run_with_timeout, capture_stderr): Not started
- Requirements 15-16 (get_remote_url, is_clean): Not started
- Requirements 17 (validate_stage_transition): Not started
- Requirements 18 (tcr_stats): Not started
- Requirements 19-20 (truncate_string, pad_left): Not started
- Requirements 21-22 (validate_slug_format, validate_priority): ✓ Done (iterations 1-2)
- Requirements 23-24 (list_worktrees, cleanup_stale_worktrees): Not started

---

## Iteration 3 (attempt 1) - IMPLEMENTER

## Summary

**What the test is asking for:**
The test `parse_args_new_with_short_slug_flag_test` requires a pure `parse_args` function that accepts a list of CLI arguments and handles the short flag `-s` as an alias for `--slug`, parsing `["new", "-s", "my_task"]` into a valid `Command`.

**What I implemented:**
1. Added `pub fn parse_args(args: List(String)) -> Result(Command, String)` - a pure, testable parser
2. Created `normalize_short_flags()` function that recursively converts `-s` to `--slug` before parsing
3. Extracted parsing logic into `do_parse()` function to avoid duplication
4. The test now passes: `cli.parse_args(["new", "-s", "my_task"])` returns `Ok(NewTask("my_task", None, False))`

**Why this approach:**
This implementation separates concerns cleanly: flag normalization is independent from argument parsing, making both functions easier to test and maintain. The short flag mapping is explicit and can easily be extended for other short flags like `-p` for `--priority`. The approach reuses existing parsing logic via `do_parse()` to avoid code duplication.

---
