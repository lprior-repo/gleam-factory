# Factory v2: Critical Problems & Solutions

**Document Purpose**: Address the 10 critical problems identified in the design document and specify exact solutions for v2

**Status**: Design phase (ready for review before implementation)

**Last Updated**: January 5, 2026

---

## Table of Contents

1. [Problem 1: The Auto-Commit Trap](#problem-1-the-auto-commit-trap)
2. [Problem 2: Confidence Scoring is Fiction](#problem-2-confidence-scoring-is-fiction)
3. [Problem 3: Learning Loop Backfires](#problem-3-learning-loop-backfires)
4. [Problem 4: Monitoring Has Lag](#problem-4-monitoring-has-lag)
5. [Problem 5: Audit Trail Confusion](#problem-5-audit-trail-confusion)
6. [Problem 6: Code Boundary Enforcement Missing](#problem-6-code-boundary-enforcement-missing)
7. [Problem 7: Coverage Metrics Meaningless](#problem-7-coverage-metrics-meaningless)
8. [Problem 8: Error Output Noise](#problem-8-error-output-noise)
9. [Problem 9: Language Agnosticism Broken](#problem-9-language-agnosticism-broken)
10. [Problem 10: Rollback Boundaries Unclear](#problem-10-rollback-boundaries-unclear)

---

## Problem 1: The Auto-Commit Trap

### What's Wrong

Auto-committing based on self-scored confidence is **recursive self-approval**:
- I define what "confidence" means
- I calculate the score
- I authorize my own deployment
- System subconsciously teaches me to optimize confidence scores

### The Honest Truth

You cannot have genuine autonomous AI without maintaining oversight. The goal isn't "zero review time" but "faster, more informed review."

### Solution for v2

**REMOVE auto-commit entirely.** Replace with:

```yaml
# .factory/bd-52.1/metrics.yaml (generated by factory)
task: bd-52.1
timestamp: 2025-01-15T14:30:00Z

measured_metrics:
  test_coverage: 89%
  cyclomatic_complexity_delta: +2
  lines_changed: 47
  new_dependencies: 0
  files_modified:
    - src/web/handlers.gleam
    - src/web/handlers_test.gleam

contract_satisfaction:
  all_required_tests_pass: âœ“
  coverage_minimum_80: âœ“ (89%)
  no_dependencies_added: âœ“
  code_follows_patterns: âœ“

deployment_readiness:
  status: READY_FOR_DECISION
  not: READY_TO_DEPLOY (human decision required)

  you_should_review_if:
    - [ ] Introduces new concurrency pattern
    - [ ] Touches critical rate limiting path
    - [ ] Requires domain expertise for edge cases

  estimated_review_time: 8 minutes
  focus_areas:
    - Lines 15-42 (concurrent request handling)
    - Lines 87-103 (token bucket reset logic)
    - Test coverage for failure scenarios

what_factory_will_do:
  if_you_approve: "Enable feature flag to 1% users, monitor for 5 min"
  if_you_reject: "Return to bd-52.1 for rework"
  never: "Deploy without your explicit approval"

your_action_needed:
  command: "factory approve bd-52.1"
  confirmation: "Are you ready to deploy batch upload handler to 1% users?"
  after_approval: "Feature flag enabled, monitoring begins"
```

**Key Change**: System shows you everything, you make the decision. No confidence scores, no guessing.

---

## Problem 2: Confidence Scoring is Fiction

### What's Wrong

Scoring like "0.92 confidence" is precise-sounding but meaningless:
- What does 0.92 actually mean?
- How is it calculated?
- Why is it different from 0.91?

### The Honest Truth

These can be measured:
- Test coverage %
- Lines changed
- New dependencies
- Cyclomatic complexity
- Similarity to past failures

These cannot be scored:
- Whether logic is correct
- Whether edge cases are handled
- Whether it'll work under load
- Whether the AI understood the problem

### Solution for v2

**Replace confidence scores with explicit metrics dashboard:**

```yaml
# factory status bd-52.1

TASK: bd-52.1 (HTTP batch endpoint)
STATUS: ready_for_decision
STAGE: passed all automated gates

â”Œâ”€ OBJECTIVE METRICS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test Coverage:         89% (threshold: 80%) âœ“      â”‚
â”‚ Lines Changed:         47 (small âœ“)                â”‚
â”‚ New Dependencies:      0 (none âœ“)                  â”‚
â”‚ Complexity Delta:      +2 (acceptable âœ“)           â”‚
â”‚ Required Tests Pass:   5/5 âœ“                       â”‚
â”‚ Pattern Match:         8/8 âœ“                       â”‚
â”‚ Lint Clean:            Yes âœ“                       â”‚
â”‚ Type Check Clean:      Yes âœ“                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ DECISION FACTORS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Introduces new patterns:          Yes âš ï¸            â”‚
â”‚ Touches critical path:            Yes âš ï¸            â”‚
â”‚ Needs domain expertise:           Yes âš ï¸            â”‚
â”‚ Past similar failures:            0 (none) âœ“       â”‚
â”‚ Similar code exists:              Yes (follow it) âœ“â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ YOUR DECISION NEEDED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Will you approve batch handler for deployment?     â”‚
â”‚                                                    â”‚
â”‚ Review time: 8 minutes (estimated)                â”‚
â”‚ Focus areas: 3 critical sections                  â”‚
â”‚ Risk level: MEDIUM (new pattern, well tested)     â”‚
â”‚                                                    â”‚
â”‚ Options:                                           â”‚
â”‚  1. Approve (deploy to 1% users)                  â”‚
â”‚  2. Request changes (return to work)              â”‚
â”‚  3. Skip review (trust system, if confident)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Change**: You see facts, not scores. You decide based on information.

---

## Problem 3: Learning Loop Backfires

### What's Wrong

When rate limiter fails, lowering confidence for all batch tasks teaches the system to:
- Avoid suggesting batch features
- Not write better batch code
- Become more conservative, not smarter

### The Honest Truth

Real learning: when something breaks, understand exactly why and add a test that would catch it.

### Solution for v2

**Surgical learning with specific tests:**

```yaml
# Incident from production

incident:
  task: bd-52.2 (rate limiter implementation)
  date: 2025-01-15
  impact: "Rate limiter failed under concurrent load"
  detection: "Monitoring alert at 2:43 PM"
  users_affected: 1,200

root_cause_analysis:
  what_failed: "50 concurrent requests to rate limiter"
  why_failed: "Token bucket not serialized (race condition)"
  location: "src/core/rate_limiter.gleam:87"
  code_issue: "Concurrent access to bucket without mutex"
  test_gap: "Tests only covered single-threaded case"

learning_action:
  not_this: "Lower confidence for rate limiter tasks (general caution)"

  do_this: "Add specific required test"
  test_name: test_concurrent_requests_50
  simulates: "50 parallel requests to rate limiter"
  setup: |
    Send 50 concurrent rate limit requests
    Each requesting 2 tokens
    All must succeed or fail predictably
  assertions:
    - No panic or deadlock
    - Bucket state remains consistent
    - All requests counted accurately

applied_to_future_work:
  contract_rule: |
    "All rate limiter modifications require concurrent load test
     (test_concurrent_requests_50 must pass)"

  scope: "Only rate limiter tasks (not all batch tasks)"

  effect: "Next time a rate limiter task runs, factory will:
           1. Require this test to exist
           2. Fail stage 3 if test missing
           3. Prevent same bug from shipping"

learning_metrics:
  before: "0 concurrent load tests for rate limiters"
  after: "All rate limiter tasks require concurrent load test"
  prevents: "Specific deadlock bug, not vague caution"

feedback_loop:
  month_1: "0 concurrent tests"
  month_3: "2 rate limiter tasks required concurrent tests"
  month_6: "10 rate limiter tasks, 0 concurrent failures"
  result: "System learned a specific, actionable lesson"
```

**Key Change**: Learning is specific to the problem, not general caution.

---

## Problem 4: Monitoring Has Lag

### What's Wrong

Assuming monitoring will catch issues is dangerous:
- Monitoring lag: 5-60 minutes
- In that window: users hit bug, data corrupts, fraud happens
- By time you rollback, damage is done

### The Honest Truth

You need multiple layers of defense:
1. **Testing** (catches bugs before prod)
2. **Feature flags** (isolates users from bad code)
3. **Gradual rollout** (limits blast radius)
4. **Monitoring** (catches what slipped through)
5. **You reviewing metrics** (human judgment on rollout)

### Solution for v2

**Mandatory feature flag deployment with gradual rollout:**

```yaml
# Deployment configuration (required for all changes)

deployment:
  strategy: gradual_with_monitoring
  never: direct_full_deploy

  stage_1:
    name: "Canary (Internal Testing)"
    percentage: 1%
    duration: 5_minutes

    who_sees_it:
      - Internal testing environment
      - Your team (if applicable)
      - NOT production users yet

    what_we_monitor:
      - Error rate
      - Panic/crash logs
      - Database errors
      - API response times

    auto_rollback_if:
      - error_rate > baseline + 10%
      - Any panic detected
      - Any database integrity error
      - Latency spike > 50%

    human_decision:
      trigger: "After 5 minutes"
      decision: "Can we expand to 10%?"
      action_if_ok: "Expand to 10% for 30 minutes"
      action_if_problem: "Auto-rollback, investigate"

  stage_2:
    name: "Early Adopters (Internal Users)"
    percentage: 10%
    duration: 30_minutes
    requires_your_decision: true

    what_we_monitor:
      - Same as canary
      - Plus: User behavior changes
      - Plus: Performance degradation
      - Plus: Data integrity (periodic checks)

    your_decision_required:
      prompt: "After 30 min at 10%, reviewed metrics? Ready for 100%?"
      options:
        - expand_to_100
        - hold_at_10_longer
        - rollback_and_investigate

  stage_3:
    name: "Production (All Users)"
    percentage: 100%
    duration: ongoing
    requires_your_approval: true

    what_we_monitor:
      - Incident rate
      - User satisfaction metrics
      - Business metrics (applicable)

    rollback_decision:
      automatic_if: "Critical incident detected"
      manual_if: "You decide it's not working"
      instant_rollback: "Feature flag OFF (no deployment wait)"

# Feature flag mechanics

feature_flag_control:
  can_control: "factory flag batch-upload --percentage 0"
  effect: "INSTANT rollback to all users"
  latency: "< 100ms"
  no_wait_for: "Redeployment, monitoring, metrics"

# This gives you control that monitoring can't provide
```

**Key Change**: You control rollout via feature flags, not monitoring lag.

---

## Problem 5: Audit Trail Confusion

### What's Wrong

Auto-updating beads to "deployed" when system auto-commits hides who decided what.

Future you looks at beads and doesn't know: "Did I approve this? Did the system deploy it?"

### The Honest Truth

Audit trail should show: "Who did what, when, and why."

### Solution for v2

**Explicit status lifecycle with person tracking:**

```yaml
# Beads issue: bd-52.1

issue:
  id: bd-52.1
  title: "Implement batch upload HTTP handler"

status_lifecycle:
  stage: ready_for_decision

  status_history:
    - timestamp: 2025-01-15 10:30:00
      status: created
      set_by: system
      reason: "Auto-generated from intent spec (bd-52)"

    - timestamp: 2025-01-15 14:20:00
      status: ready_for_decision
      set_by: system
      reason: "All automated gates passed (10/10 stages)"
      signal: |
        Tests pass âœ“
        Coverage 89% âœ“
        Lint clean âœ“
        Type check clean âœ“
        Patterns match âœ“

    - timestamp: 2025-01-15 14:22:00
      status: approved
      set_by: lewis (you)
      command: "factory approve bd-52.1"
      decision: "Reviewed 8 min, concurrency logic looks good"
      risk_accepted: true

    - timestamp: 2025-01-15 14:25:00
      status: deployed
      set_by: lewis (you)
      command: "factory deploy bd-52.1"
      decision: "Enabling feature flag to 1% users"
      rollout_plan: "1% for 5min, then review metrics"

    - timestamp: 2025-01-15 14:26:00
      status: monitoring
      set_by: system (automatic)
      reason: "Feature flag enabled, monitoring active"

    - timestamp: 2025-01-15 14:30:00
      status: monitoring_complete
      set_by: system (automatic)
      metrics:
        error_rate: "normal"
        latency: "normal"
        decision: "OK to expand to 10%"

    - timestamp: 2025-01-15 14:32:00
      status: deployed_10_percent
      set_by: system (your approval from stage 2)
      note: "Expanding to 10% (30 min)"

    - timestamp: 2025-01-15 15:02:00
      status: ready_for_production
      set_by: system
      reason: "10% monitoring complete, ready for full deploy"

    - timestamp: 2025-01-15 15:03:00
      status: deployed_100_percent
      set_by: lewis (you)
      decision: "Metrics look good, expanding to all users"

# Audit trail shows
what_system_can_do: "Automated gates, monitoring, decisions"
what_you_did: "Approved, deployed, approved expansion"
what_was_automatic: "Clear distinction"

# Future you can see exactly:
who_decided: you
when_decided: 15:03:00
why_decided: "Metrics looked good at 10%"
what_happened: "Feature deployed to 100%"
# No ambiguity
```

**Key Change**: Clear audit trail of human vs system decisions.

---

## Problem 6: Code Boundary Enforcement Missing

### What's Wrong

Contract says "bd-52.1 can ONLY touch handlers.gleam" but there's no enforcement.

I could accidentally (or intentionally) modify rate_limiter.gleam and commit it.

### The Honest Truth

Boundaries must be enforced before commit, not discovered after.

### Solution for v2

**Pre-commit hook with contract validation:**

```yaml
# .factory/bd-52.1/contract.yaml

contract:
  task: bd-52.1
  title: "Implement batch upload HTTP handler"

  boundary:
    allowed_files:
      - src/web/handlers.gleam
      - src/web/handlers_test.gleam
      - test/fixtures/batch_requests.json

    forbidden_files:
      - src/core/rate_limiter.gleam      # That's bd-52.2
      - src/core/validator.gleam         # That's bd-52.3
      - src/infra/db.gleam               # That's bd-52.4
      - src/worker/pool.gleam            # Different epic

  test_requirements:
    - test_POST_batch_endpoint_accepts_urls
    - test_batch_id_returned_immediately
    - test_empty_array_rejected
    - test_invalid_url_rejected
    - test_concurrent_requests_handled

# Pre-commit hook (.git/hooks/pre-commit)

script: |
  #!/bin/bash

  TASK_ID="bd-52.1"
  CONTRACT_FILE=".factory/${TASK_ID}/contract.yaml"

  # Get list of files being committed
  STAGED_FILES=$(git diff --cached --name-only)

  # Load allowed/forbidden files from contract
  ALLOWED=$(grep -A 10 "allowed_files:" $CONTRACT_FILE | grep "- " | cut -d' ' -f4)
  FORBIDDEN=$(grep -A 10 "forbidden_files:" $CONTRACT_FILE | grep "- " | cut -d' ' -f4)

  # Check each file
  for file in $STAGED_FILES; do
    # Is this file allowed?
    if echo "$FORBIDDEN" | grep -q "^${file}$"; then
      echo "âœ— BLOCKED: ${TASK_ID} cannot modify $file"
      echo "âœ— Reason: That's another task's responsibility"
      echo "âœ— Allowed files:"
      echo "$ALLOWED" | sed 's/^/  - /'
      exit 1
    fi
  done

  exit 0

# What happens when you try to violate boundary

user_action: "git add src/core/rate_limiter.gleam"
pre_commit_hook_fires: true

output: |
  âœ— BLOCKED: bd-52.1 cannot modify src/core/rate_limiter.gleam
  âœ— Reason: Rate limiting is bd-52.2's responsibility
  âœ— Allowed files:
    - src/web/handlers.gleam
    - src/web/handlers_test.gleam
    - test/fixtures/batch_requests.json
  âœ— Commit aborted

result: You MUST either:
  1. Remove that file from commit
  2. Create new issue (bd-52.5) for scope creep
  3. Or update the contract if it's wrong

# System prevents scope creep before it happens
```

**Key Change**: Boundaries enforced by pre-commit hook, not hope.

---

## Problem 7: Coverage Metrics Meaningless

### What's Wrong

"Coverage: 87% âœ“" tells you almost nothing.

What if the 13% uncovered is the entire rate limiting logic?

### The Honest Truth

Coverage should show: "Here's what's tested, here's what's not, and what's critical."

### Solution for v2

**Contextual coverage analysis:**

```yaml
# Coverage report (generated by factory stage 4)

coverage_analysis:
  task: bd-52.1
  timestamp: 2025-01-15T14:30:00Z

  overall_statistics:
    lines_covered: 187
    lines_total: 216
    percentage: 87%
    threshold_required: 80%
    status: âœ“ PASS

  critical_paths_analysis:
    # Not just "what % is covered"
    # But: "Are the important parts tested?"

    path: "Happy path (valid batch of 100 URLs)"
    coverage: 100% âœ“
    why_critical: "Most common scenario"

    path: "Rate limit enforcement (>100 URLs)"
    coverage: 45% âš ï¸
    why_critical: "Prevents abuse"
    missing_scenarios:
      - Concurrent requests to same rate limiter
      - Rate limit reset timing
      - Edge cases at exactly 100 URLs
    action_needed: "Add concurrent load test (test_concurrent_requests_50)"

    path: "Error handling (invalid URLs)"
    coverage: 92% âœ“
    why_critical: "Prevents crashes"

    path: "Database failure recovery"
    coverage: 0% ðŸ”´
    why_critical: "Prevents data loss"
    missing: "Entire error recovery path untested"
    action_needed: "Add db-failure test"
    blocking: "Must be tested before production"

  by_function:
    - function: handle_batch_POST
      coverage: 95%
      tested_scenarios: 12

    - function: rate_limit_check
      coverage: 45%
      tested_scenarios: 2
      missing_scenarios: ["concurrent requests", "reset timing", "boundary cases"]

    - function: validate_url
      coverage: 92%
      tested_scenarios: 8

    - function: db_insert_batch
      coverage: 78%
      tested_scenarios: 5
      missing_scenarios: ["db failure recovery"]

  recommendation:
    auto_commit_ready: NO
    reason: "Critical paths undertested"
    specific_actions:
      - Add: test_concurrent_requests_50
      - Add: test_db_failure_recovery
      - Add: test_rate_limit_boundary_cases

    retry_after:
      "Once these tests added:"
      "- Coverage will be 94%+"
      "- Critical paths will be 95%+"
      "- Then ready for deployment"

# This coverage report has actionable information
# Not just a percentage, but a plan
```

**Key Change**: Coverage shows what's important and what's missing.

---

## Problem 8: Error Output Noise

### What's Wrong

When stage 3 fails, output includes:
- 100+ lines of crash dumps
- Erlang process trees
- Deep stack traces
- Actual error buried in noise

### The Honest Truth

Developers need the first actual error highlighted, with context.

### Solution for v2

**Error extraction and summarization:**

```yaml
# Stage output (from factory run bd-52.1)

[ 3/10] unit-test

âœ— FAILED after 3 retries

â”Œâ”€ ROOT CAUSE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test: test_concurrent_requests_50                 â”‚
â”‚ Location: src/web/handlers_test.gleam:127        â”‚
â”‚ Error: panic: race condition in rate limiter     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ WHAT WENT WRONG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Two concurrent requests tried to access the      â”‚
â”‚ token bucket without proper synchronization.     â”‚
â”‚                                                   â”‚
â”‚ Thread 1: Checking available tokens              â”‚
â”‚ Thread 2: Also checking (reads stale data)       â”‚
â”‚ Result: Both threads granted token, exceeding    â”‚
â”‚           rate limit                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ YOUR OPTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                   â”‚
â”‚ 1. FIX: Implement mutex for token bucket access  â”‚
â”‚    Then: factory stage bd-52.2 unit-test         â”‚
â”‚                                                   â”‚
â”‚ 2. ASK FOR HELP: Claude, why does this fail?     â”‚
â”‚    Then: factory explain bd-52.2                 â”‚
â”‚                                                   â”‚
â”‚ 3. TCR WAS AUTOMATIC: Changes have been revertedâ”‚
â”‚    Status: Back to last known good state         â”‚
â”‚    Then: Approach differently                    â”‚
â”‚                                                   â”‚
â”‚ 4. DEBUG: Run with verbose output                â”‚
â”‚    Then: factory debug bd-52.2 unit-test         â”‚
â”‚                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ FULL LOG (if you need deep details) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Saved to: .factory/bd-52.2/stage-3-failure.log  â”‚
â”‚ Size: 2.3 MB (300 lines of actual error output)  â”‚
â”‚ Contains: Full stack trace, thread info, etc.   â”‚
â”‚                                                   â”‚
â”‚ View with: cat .factory/bd-52.2/stage-3-failure.â”‚
â”‚            log | tail -100                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TCR: Automatic revert triggered
â”œâ”€ Command: jj restore --from @-
â”œâ”€ Result: Changes reverted to parent state
â”œâ”€ Your code: Rolled back to last test-passing state
â””â”€ Next: Fix and retry (or rethink approach)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What factory did:
âœ“ Identified root cause
âœ“ Explained what went wrong
âœ“ Showed your options
âœ“ Provided full log for deep dive
âœ“ Automatically reverted changes (TCR)

What you should do:
1. Fix the race condition (add mutex)
2. Run tests again
3. Or ask for help if stuck
```

**Key Change**: Error messages are helpful, not overwhelming.

---

## Problem 9: Language Agnosticism Broken

### What's Wrong

Factory hardcoded for Go (gofmt, go vet).

Video-puller is Gleam â†’ stages fail or run wrong commands.

### The Honest Truth

Factory must auto-detect language and generate correct stage commands.

### Solution for v2

**Language detection and auto-configuration:**

```yaml
# When you create workspace

factory new batch-upload --from bd-52.1

# System detects language by looking for:
# 1. Language manifest files
language_detection:
  check_1_gleam_toml: "/home/lewis/src/factory-gleam/gleam.toml"
  found: true
  language: gleam
  version: 0.44

  check_2_go_mod: not_found
  check_3_cargo_toml: not_found
  check_4_pyproject_toml: not_found

detected_language: gleam

# Auto-generate .factory/gleam.yaml (language config)
config_file_generated: ".factory/gleam.yaml"

content: |
  # Auto-generated for Gleam 0.44
  # Based on: gleam.toml detection

  language: gleam
  version: 0.44

  stages:
    implement:
      command: "gleam check && gleam build"
      expected_success_output:
        - "âœ“ Compiling"
        - "âœ“ Checking"
      success_means: "exit_code == 0"
      failure_patterns:
        - "Error:"
        - "Compilation error"
      retries: 5
      tcr: true

    unit_test:
      command: "gleam test"
      expected_success_output:
        - "test result: ok"
      success_means: "exit_code == 0 AND contains 'ok'"
      failure_patterns:
        - "test result: fail"
        - "assertion failed"
      retries: 3
      tcr: true

    coverage:
      command: "gleam test --coverage"
      parse_output: "extract coverage_percent from JSON"
      success_means: "coverage_percent >= 80"
      retries: 5
      tcr: true

    lint:
      command: "gleam format --check"
      expected_success_output:
        - "All formatted"
      success_means: "exit_code == 0"
      failure_patterns:
        - "Unformatted files"
      retries: 3
      tcr: true

    static:
      command: "gleam check"
      expected_success_output:
        - "0 errors"
      success_means: "exit_code == 0"
      failure_patterns:
        - "Type error"
        - "Error:"
      retries: 3
      tcr: true

  # Gleam-specific quirks
  language_quirks:
    - name: "Type checking integrated with build"
      implication: "Don't separate compile from type check"

    - name: "Tests use ?module prefix"
      implication: "Watch for import errors in test setup"

    - name: "Coverage tool outputs JSON"
      implication: "Must parse JSON not text output"

# What factory does with this:

stages:
  - name: implement
    command: "gleam check && gleam build"  # Gleam, not Go
    ...

  - name: unit-test
    command: "gleam test"  # Gleam, not Go
    ...

# Supported languages (auto-detect from files)

languages_supported:
  gleam:
    manifest_file: gleam.toml
    config_template: .factory/gleam.yaml

  go:
    manifest_file: go.mod
    config_template: .factory/go.yaml

  rust:
    manifest_file: Cargo.toml
    config_template: .factory/rust.yaml

  python:
    manifest_file: pyproject.toml
    config_template: .factory/python.yaml

# Result: Factory works for any language
```

**Key Change**: Auto-detect language, generate correct config.

---

## Problem 10: Rollback Boundaries Unclear

### What's Wrong

TCR says "revert" but doesn't define:
- To what state?
- Using what command?
- What exactly gets undone?

### The Honest Truth

Rollback must be explicit, documented, and automatic.

### Solution for v2

**Explicit TCR specifications:**

```yaml
# TCR Definition in contract

contract:
  task: bd-52.1

  tcr_behavior:
    what_is_tcr: |
      Test && Commit || Revert

      If tests pass:
        - Commit changes to branch
        - Create new change for next work
        - Continue pipeline

      If tests fail (all retries exhausted):
        - Revert to last known-good state
        - Do NOT commit broken code
        - Return error, let you fix it

    applies_to_stages:
      - stage: implement
        tcr: true
        reason: "Code must compile"

      - stage: unit-test
        tcr: true
        reason: "Tests must pass"

      - stage: coverage
        tcr: true
        reason: "Coverage must meet threshold"

      - stage: lint
        tcr: true
        reason: "Code must be formatted"

      - stage: static
        tcr: true
        reason: "Type checks must pass"

      - stage: integration
        tcr: true
        reason: "Integration tests must pass"

      - stage: security
        tcr: true
        reason: "Security scans must pass"

      - stage: review
        tcr: true
        reason: "Code review must pass"

      - stage: tdd-setup
        tcr: false
        reason: "Tests must FAIL at start (red state)"

      - stage: accept
        tcr: false
        reason: "Final gate, no auto-revert"

# Explicit rollback mechanism

rollback:
  mechanism: "jj restore"
  what_it_does:
    - Reverts all file changes
    - Returns to parent change
    - Undoes all work in current change

  command: "jj restore --from @-"

  target:
    current_state: "@" (latest change)
    restore_to: "@-" (parent change)
    meaning: "Go back to state before current work started"

  automatic_trigger:
    - Stage fails after all retries exhausted
    - TCR enabled for that stage
    - System automatically runs: jj restore --from @-

  you_will_see:
    output: |
      âœ“ TCR triggered: Stage failed
      âœ“ Reverting changes: jj restore --from @-
      âœ“ Reverted files:
        - src/web/handlers.gleam
        - src/web/handlers_test.gleam
      âœ“ Status: Back to last known good state

# Example flow

example_sequence:
  step_1:
    description: "Handler endpoint working, tests passing"
    jj_state: "@" (let's call this change ABC123)
    command: "git status"
    result: "All tests passing"

  step_2:
    description: "Add rate limiting to handler"
    action: "Modify handlers.gleam to call rate_limiter"
    jj_state: "@" (same change ABC123, accumulating)
    command: "factory stage bd-52.1 unit-test"
    result: "Tests still pass"

  step_3:
    description: "Add concurrent validation"
    action: "Modify handlers.gleam for concurrent requests"
    jj_state: "@" (same change ABC123, accumulating)
    command: "factory run bd-52.1"

  step_4:
    description: "Stage 3 (unit-test) fails"
    error: "Concurrent access causes race condition"
    output: |
      [ 3/10] unit-test
      âœ— FAILED: test_concurrent_requests_50
      Attempts: 1/3, 2/3, 3/3 all failed

  step_5:
    description: "All retries exhausted, TCR triggers"
    command: "jj restore --from @-"
    result: |
      âœ“ Restoring to parent change
      âœ“ Reverted files:
        - src/web/handlers.gleam
        - src/web/handlers_test.gleam
      âœ“ Status: Back to state from step 1
      âœ“ Handler still working, tests still passing
      âœ“ Rate limiting attempt reverted

  step_6:
    description: "What you do next"
    option_1: "Fix the race condition and retry"
    option_2: "Ask for help understanding the issue"
    option_3: "Break work into smaller pieces"
    option_4: "Review the test failure log"

# Key insight:
# TCR ensures: No broken code ever persists
# If stage fails: Changes are gone
# You start fresh with clean state
# Try again, or try different approach
```

**Key Change**: Rollback is explicit, automatic, and transparent.

---

## Summary Table: All 10 Problems & Solutions

| Problem | Root Cause | v1 Issue | v2 Solution |
|---------|-----------|---------|------------|
| 1. Auto-Commit Trap | Recursive self-approval | System scores itself | Remove auto-commit, YOU decide |
| 2. Confidence is Fiction | Meaningless precision | 0.92 score = guess | Show metrics, you judge |
| 3. Learning Backfires | General caution lowering | System becomes conservative | Surgical learning: add specific test |
| 4. Monitoring Lag | 5-60 min gap | Users hit bugs before rollback | Feature flags (instant control) |
| 5. Audit Confusion | System auto-updates statuses | Future you doesn't know if you approved | Explicit statuses, person tracking |
| 6. No Boundary Enforce | Hope doesn't scope creep | Can accidentally touch other files | Pre-commit hook enforcement |
| 7. Coverage Meaningless | Just percentage | 87% = "looks OK?" | Critical paths analysis |
| 8. Error Noise | Raw output from tools | Buried errors | Extract + summarize |
| 9. Language Agnostic | Hardcoded for Go | Wrong commands for Gleam | Auto-detect, generate config |
| 10. Rollback Unclear | Documentation gap | Uncertain what gets undone | Explicit jj commands |

---

## Implementation Priorities for v2

### Phase 1 (Blocking): Safety Guarantees

These MUST be implemented before anything else:
1. **Problem 1**: Remove auto-commit (CRITICAL)
2. **Problem 4**: Feature flag mandatory for all deploys (CRITICAL)
3. **Problem 6**: Pre-commit hook boundary enforcement (CRITICAL)
4. **Problem 5**: Explicit status lifecycle (IMPORTANT)

Without these, the system can cause harm.

### Phase 2 (Core): Factory Functionality

These enable the actual pipeline:
5. **Problem 8**: Error extraction and summarization (CRITICAL for UX)
6. **Problem 9**: Language detection and auto-config (CRITICAL for flexibility)
7. **Problem 10**: Explicit TCR specifications (IMPORTANT for clarity)

### Phase 3 (Polish): Quality Assurance

These improve decision-making:
8. **Problem 2**: Metrics dashboard (IMPORTANT for transparency)
9. **Problem 7**: Coverage analysis (IMPORTANT for understanding)
10. **Problem 3**: Surgical learning (NICE to have, adds value over time)

### Phase 4 (Integration): Tool Chain

Once factory is solid:
- Moon.dev integration (factory calls moon)
- Beads integration (issues link to workspaces)
- Codanna integration (pattern detection)
- Intent integration (spec-driven decomposition)

---

## Success Criteria for v2

### Safety Criteria (Must Have)

- [ ] No auto-commits (only manual approval)
- [ ] Feature flags mandatory for all deployments
- [ ] Pre-commit hooks prevent scope creep
- [ ] Audit trail shows who approved what, when
- [ ] No broken code ever persists (TCR ensures this)

### Functionality Criteria (Must Have)

- [ ] Language auto-detection works for Gleam, Go, Rust, Python
- [ ] All 10 stages run with correct language-specific commands
- [ ] Error messages are clear and actionable
- [ ] Rollback via TCR is automatic and transparent
- [ ] Factory integrates with moon.dev

### Transparency Criteria (Nice to Have)

- [ ] Metrics dashboard shows objective measurements
- [ ] Coverage analysis shows critical vs non-critical paths
- [ ] Surgical learning: incidents create specific test requirements
- [ ] Beads status shows clear human vs system decisions

---

## What's NOT Changing

To be clear, these stay the same:

- **Jujutsu workspaces**: Still best for isolation and speed
- **TCR discipline**: Still core enforcement mechanism
- **10-stage pipeline**: Still gold standard
- **Contract rigidity**: Still prevents bad changes
- **Codanna integration**: Still enables pattern matching
- **Intent specs**: Still source of truth

---

## Next Steps

1. **Review this document** - Does it accurately capture the problems and solutions?
2. **Provide feedback** - Any modifications or clarifications needed?
3. **Approve implementation order** - Agree on Phase 1-4 priorities?
4. **Begin Phase 1** - Start with safety guarantees before anything else?

This v2 design maintains the best of v1 while fixing the critical problems you identified.

